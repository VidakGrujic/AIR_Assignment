{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0528baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9016d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        return data['data']\n",
    "    \n",
    "training_dataset_path = \"datasets/training/parsed_data_final.json\"\n",
    "test_dataset = \"results/bm25_results_training.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df23a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_articles_for_question(question):\n",
    "    return question.get('ground_truth_documents_pid', [])\n",
    "\n",
    "def get_ground_truth_snippets_for_question(question):\n",
    "    return question.get('ground_truth_snippets', [])\n",
    "\n",
    "def get_top_10_articles(question):\n",
    "    return question.get('top_10_articles', [])\n",
    "\n",
    "def get_top_10_snippets(question):\n",
    "    return question.get('snippets', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d81e95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this computes MRR for single quesion\n",
    "def compute_mrr(predicted_pids, ground_truth_pids):\n",
    "    for i, pid in enumerate(predicted_pids):\n",
    "        if pid in ground_truth_pids:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ac8c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this computes average precision for single question\n",
    "def compute_average_precision(predicted_pids, ground_truth_pids):\n",
    "    \"\"\"\n",
    "    Computes Average Precision based on:\n",
    "    AP = sum(P@i * rel_i) / |relevant documents|\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    score = 0.0\n",
    "    for i, pid in enumerate(predicted_pids):\n",
    "        if pid in ground_truth_pids:\n",
    "            hits += 1\n",
    "            score += hits / (i + 1)  # Precision at rank i\n",
    "    return score / len(ground_truth_pids) if hits > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a34820b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes nDCG for single question\n",
    "def compute_ndcg(predicted_pids, ground_truth_pids, k=10):\n",
    "    \"\"\"\n",
    "    nDCG with binary relevance (1 if in ground truth, 0 otherwise)\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    for i, pid in enumerate(predicted_pids[:k]):\n",
    "        if pid in ground_truth_pids:\n",
    "            dcg += 1 / math.log2(i + 2)  # i+2 because ranks are 1-based\n",
    "\n",
    "    ideal_dcg = sum(1 / math.log2(i + 2) for i in range(min(len(ground_truth_pids), k)))\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d82b1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics_for_articles(ground_truth_data, predicted_data, k=10):\n",
    "    mrr_total = 0.0\n",
    "    map_total = 0.0\n",
    "    ndcg_total = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for gt_question, pred_question in tqdm(zip(ground_truth_data, predicted_data), desc=\"Processing questions...\"):\n",
    "        gt_pids = set(get_ground_truth_articles_for_question(gt_question))\n",
    "        pred_pids = [val['pid'] for val in get_top_10_articles(pred_question)]\n",
    "\n",
    "        print(\"GT:\", gt_pids)\n",
    "        print(\"Pred:\", pred_pids)\n",
    "\n",
    "\n",
    "\n",
    "        if not gt_pids:\n",
    "            continue\n",
    "\n",
    "        mrr_total += compute_mrr(pred_pids, gt_pids)\n",
    "        map_total += compute_average_precision(pred_pids, gt_pids)\n",
    "        ndcg_total += compute_ndcg(pred_pids, gt_pids, k=k)\n",
    "        count += 1\n",
    "\n",
    "        if count == 0:\n",
    "            return {'MRR': 0.0, 'MAP': 0.0, f\"nDCG@{k}\": 0.0}\n",
    "        \n",
    "        return {\n",
    "            \"MRR\": mrr_total / count,\n",
    "            \"MAP\": map_total / count,\n",
    "            f\"nDCG@{k}\": ndcg_total / count\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "115ba880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions...: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: {'http://www.ncbi.nlm.nih.gov/pubmed/15858239', 'http://www.ncbi.nlm.nih.gov/pubmed/12239580', 'http://www.ncbi.nlm.nih.gov/pubmed/15829955', 'http://www.ncbi.nlm.nih.gov/pubmed/6650562', 'http://www.ncbi.nlm.nih.gov/pubmed/20598273', 'http://www.ncbi.nlm.nih.gov/pubmed/21995290', 'http://www.ncbi.nlm.nih.gov/pubmed/23001136', 'http://www.ncbi.nlm.nih.gov/pubmed/15617541', 'http://www.ncbi.nlm.nih.gov/pubmed/8896569'}\n",
      "Pred: ['http://www.ncbi.nlm.nih.gov/pubmed/15858239', 'http://www.ncbi.nlm.nih.gov/pubmed/6650562', 'http://www.ncbi.nlm.nih.gov/pubmed/15829955', 'http://www.ncbi.nlm.nih.gov/pubmed/12239580', 'http://www.ncbi.nlm.nih.gov/pubmed/8896569', 'http://www.ncbi.nlm.nih.gov/pubmed/37522903', 'http://www.ncbi.nlm.nih.gov/pubmed/7719019', 'http://www.ncbi.nlm.nih.gov/pubmed/15956201', 'http://www.ncbi.nlm.nih.gov/pubmed/36742534', 'http://www.ncbi.nlm.nih.gov/pubmed/23001136']\n",
      "{'MRR': 1.0, 'MAP': 0.6222222222222222, 'nDCG@10': 0.7609655944414468}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth_data = load_json(training_dataset_path)\n",
    "predicted_data = load_json(test_dataset)\n",
    "\n",
    "results = evaluate_metrics_for_articles(ground_truth_data, predicted_data)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
